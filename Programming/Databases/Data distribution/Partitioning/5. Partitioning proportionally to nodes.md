Used by Cassandra and Ketama. Th approach is to make the number of partitions proportional to the number of nodes—in other words, to have a fixed number of par‐ titions per node.

In this case, the size of each partition grows proportionally to the dataset size while the number of nodes remains unchanged, but when you increase the number of nodes, the partitions become smaller again. Since a larger data volume generally requires a larger number of nodes to store, this approach also keeps the size of each partition fairly stable.

When a new node joins the cluster, it randomly chooses a fixed number of existing partitions to split, and then takes ownership of one half of each of those split partitions while leaving the other half of each partition in place. The randomization can produce unfair splits, but when averaged over a larger number of partitions (in Cassandra, 256 partitions per node by default), the new node ends up taking a fair share of the load from the existing nodes. Cassandra 3.0 introduced an alternative rebalancing algorithm that avoids unfair splits

Picking partition boundaries randomly requires that hash-based partitioning is used - so the boundaries can be picked from the range of numbers produced by the hash function. Indeed, this approach corresponds most closely to the original definition of consistent hashing