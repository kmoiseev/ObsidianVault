**partition**:  aka a *shard* in MongoDB, Elasticsearch, and SolrCloud; *region* in HBase, a *tablet* in Bigtable, a *vnode* in Cassandra and Riak, and a *vBucket* in Couchbase.

Normally, partitions are defined in such a way that each piece of data (each record, row, or document) belongs to exactly one partition. In effect, each partition is a small database of its own, although the database may support operations that touch multiple partitions at the same time.

The main reason for wanting to partition data is **scalability**. Different partitions can be placed on different nodes in a shared-nothing cluster. Thus, a large dataset can be distributed across many disks, and the query load can be distributed across many processors.

For queries that operate on a single partition, each node can independently execute the queries for its own partition, so query throughput can be scaled by adding more nodes. Large, complex queries can potentially be parallelized across many nodes, although this gets significantly harder.

Partitioning is usually combined with replication so that copies of each partition are stored on multiple nodes. This means that, even though each record belongs to exactly one partition, it may still be stored on several different nodes for fault toler‚Äê ance.

![[DB_Partitioning_Combining_partition_leaders_and_followers.png]]

Main goal with partitioning is to *spread the data and the query load evenly across nodes*.

If the partitioning is unfair, so that some partitions have more data or queries than others, it is called *skewed*. A partition with disproportionately high load is called a *hot spot*.



