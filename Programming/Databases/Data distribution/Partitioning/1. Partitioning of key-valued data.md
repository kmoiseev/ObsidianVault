The simplest approach for avoiding hot spots would be to assign records to nodes randomly. That would distribute the data quite evenly across the nodes, but it has a big disadvantage: when you’re trying to read a particular item, you have no way of knowing which node it is on, so you have to query all nodes in parallel.

## Partitioning by Key Range

One way of partitioning is to assign a continuous range of keys (from some minimum to some maximum). If you know the boundaries between the ranges, you can easily determine which partition contains a given key. If you also know which partition is assigned to which node, then you can make your request directly to the appropriate node.

![[DB_Partition_By_key_range.png]]

The partition boundaries might be chosen manually by an administrator, or the database can choose them automatically.

This partitioning strategy is used by Bigtable, its open source equivalent HBase, RethinkDB, and MongoDB before version 2.4

Within each partition, we can keep keys in sorted order

Pros:
Easily range search by the key

Cons:
Likely to cause *skewed* partition 

## Partitioning by Hash of Key
Many distributed datastores use a hash function to determine the partition for a given key.

For partitioning purposes, the hash function need not be cryptographically strong: for example, Cassandra and MongoDB use MD5, and Voldemort uses the Fowler-Noll–Vo function.

Once you have a suitable hash function for keys, you can assign each partition a range of hashes.
![[DB_Partition_By_key_hash_range.png]]
Unfortunately, by using the hash of the key for partitioning we lose a nice property of key-range partitioning: the ability to do efficient range queries.

Keys that were once adjacent are now scattered across all the partitions, so their sort order is lost. In MongoDB, if you have enabled hash-based sharding mode, any range query has to be sent to all partitions.