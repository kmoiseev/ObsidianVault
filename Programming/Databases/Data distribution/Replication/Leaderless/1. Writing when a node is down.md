 In case one replica is not available, in a leader-based configuration that would require performing the failover procedure. In leaderless replication failover does not exist. When node comes back online and clients start reading from - the data returned might be staled (outdated).
 ![[DB_Replication_Leaderless_node_offline.png]]
 To solve that problem, when a client reads from the database, it doesnâ€™t just send its request to one replica: *read requests are also sent to several nodes in parallel*. The client may get different responses from different nodes; i.e., the up-to-date value from one node and a stale value from another. Version numbers are used to determine which value is newer.
 
To catch-up the node getting back online, two mechanisms are often used in Dynamo-style datastores :
 

*Read repair*

When a client makes a read from several nodes in parallel, it can detect any stale responses. The client sees that a replica has a stale value and writes the newer value back to that replica. This approach works well for values that are frequently read.

*Anti-entropy process*

In addition, some datastores have a background process that constantly looks for differences in the data between replicas and copies any missing data from one replica to another. Unlike the replication log in leader-based replication, this anti-entropy process does not copy writes in any particular order, and there may be a significant delay before data is copied.